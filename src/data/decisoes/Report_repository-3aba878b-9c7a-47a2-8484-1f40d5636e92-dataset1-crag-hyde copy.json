{
    "id": "3aba878b-9c7a-47a2-8484-1f40d5636e92",
    "query_set_name": "decisoes_chunks",
    "llm_model": "gemini",
    "source": "decisoes_chunks7",
    "created_at": "2024-10-28T02:30:26.428368",
    "categories": {
        "direct-query": {
            "id": "3efaa838-c0e2-48ef-b470-1572c7bdfd9e",
            "category_name": "direct-query",
            "interactions": {
                "simple-query": {
                    "metrics": {
                        "faithfulness": 1.0,
                        "context_recall": 0.0,
                        "llm_context_precision_without_reference": 0.0,
                        "context_entity_recall": 0.0,
                        "answer_relevancy": 0.0
                    }
                },
                "complex-query": {

                    "metrics": {
                        "faithfulness": 0.0,
                        "context_recall": 0.0,
                        "llm_context_precision_without_reference": 0.0,
                        "context_entity_recall": 0.06666666662222222,
                        "answer_relevancy": 0.8603771581112815
                    }
                },
                "comparion-query": {

                    "metrics": {
                        "faithfulness": 0.075,
                        "context_recall": 0.16666666666666666,
                        "llm_context_precision_without_reference": 0.0,
                        "context_entity_recall": 0.0,
                        "answer_relevancy": 0.8871142926167869
                    }
                },
                "multi-hop-query": {

                    "metrics": {
                        "faithfulness": 0.75,
                        "context_recall": 1.0,
                        "llm_context_precision_without_reference": 0.49999999995,
                        "context_entity_recall": 0.49999999875,
                        "answer_relevancy": 0.903623205012039
                    }
                },
                "open_query": {

                    "metrics": {
                        "faithfulness": 0.075,
                        "context_recall": 0.1875,
                        "llm_context_precision_without_reference": 0.841225749549736,
                        "context_entity_recall": 0.09090909082644627,
                        "answer_relevancy": 0.8749347125087983
                    }
                }
            }
        }
    },
    "overall_metrics": {
        "faithfulness": 0.38,
        "context_recall": 0.27083333333333337,
        "llm_context_precision_without_reference": 0.2682451498999472,
        "context_entity_recall": 0.1315151512397337,
        "answer_relevancy": 0.7052098736497812
    }
}